Hypothesis ID: 0
Averaged Score: 4.5; Scores: [5, 4, 4, 5]
Number of rounds: 3
We hypothesize that a modular, hybrid generative model for crystal structures—combining alternating space-group-conditioned sequence and set-equivariant blocks, online legality prediction, and rigorous canonicalization—will faithfully reproduce the empirical distribution of symmetry space groups and outperform state-of-the-art models in stability, uniqueness, and novelty. 

1. Data Representation & Canonicalization: Crystal structures are encoded via (space group, Wyckoff position sequences, atomic species, and positional parameters). Canonicalization enforces uniqueness using a lex-minimization algorithm over Wyckoff label permutations and atomic site order, implemented with crystallographic libraries (e.g., spglib/pymatgen) and a deterministic tiebreaker protocol for partial occupancies and ambiguous cases. Structures that cannot be uniquely canonicalized (e.g., unresolved disorder) are flagged for exclusion in uniqueness evaluation and further studied for robustness.

2. Model Architecture: The generative model stacks alternating blocks: (a) sequence modules (transformer layers conditioned on space group embeddings, enabling group-specialized attention) and (b) set-equivariant layers (e.g., e3nn or set transformer) operating on Wyckoff and atomic site sets. Interleaved cross-attention enables explicit message passing between sequence and set representations. Group heads share parameters via a conditional embedding strategy, allowing scalability across frequent and rare space groups.

3. Legality Masking: At each generation step, permitted Wyckoff moves are enforced via the differentiable union of hard-rule masks (from crystallographic constraints) and MLP-based learned legality predictors. Gradients flow through the MLP branch, supporting end-to-end training. If no legal moves are available, generation terminates early and the attempt is flagged for data augmentation.

4. Objective Functions: The model is trained with: (a) symmetry-aware NLL for autoregressive outputs, (b) batch-wise uniqueness loss penalizing symmetry-equivalent duplicates, (c) distribution-matching Earth Mover’s Distance loss for generated vs. empirical Wyckoff/space-group frequencies, and (d) optional auxiliary losses for structure stability or other properties.

5. Training, Evaluation, and Ablation: The protocol includes fully specified preprocessing (using MP-20 or relevant materials datasets), train/validation/test/OOD split logic (with rare group stratification), full subroutine pseudocode (canonicalization, legality masking, sampling, augmentation), and documented axes of ablation (fusion logic, legality enforcement, group specialization, loss components). Hyperparameters, sampling criteria, and resource constraints are described in tables with recommended ranges.

This architecture and protocol are generalizable beyond inorganic crystals and designed for tractable, ablation-ready, reproducible benchmarking, with all operations clearly defined and provisioned for community adoption.


Hypothesis ID: 1
Averaged Score: 4.5; Scores: [5, 4, 4, 5]
Number of rounds: 2
Hypothesis: A generative model that explicitly and canonically encodes crystalline structures as permutation-pruned Wyckoff site tuples—paired with SE(3)-equivariant latent representations for asymmetric unit coordinates and trained with symmetry-group-conditioned, permutation/orbit-invariant losses—will faithfully reproduce the symmetry space group distribution and generate uniquely stable, novel materials, surpassing previous models on stability, uniqueness, and symmetry metrics.

Methodology:
1. **Data Encoding**
   - For each crystal, extract the space group and list of (Wyckoff letter, site multiplicity, element) using pymatgen and spglib. Canonical tuple is lex-minimized over all symmetry-equivalent Wyckoff orderings, e.g., for space group 221: [('a',1,Ca),('b',1,Ti),('c',3,O)] becomes [('a',1,Ca),('b',1,Ti),('c',3,O)]. Only a unique tuple is kept for each structure.
   - Partial occupancies/disorder are discretized into multi-label entries or filtered as appropriate.
   - Apply on-the-fly group-theoretical symmetry augmentation: each batch contains N (e.g., 8–16) random symmetry-augmented variants.
2. **Model Architecture**
   - Discrete generator (conditional autoregressive transformer): predicts Wyckoff site tuple, conditioned on group embedding (size 128), using 4 transformer layers (8 heads, 256-dim emb).
   - Continuous generator (SE(3)-equivariant GNN): inputs predicted tuple and generates asymmetric unit coordinates, with group-specific conditioning (as embedding, concatenated to node features). 5 graph layers, 128-dim node states.
   - Joint latent space is regularized for mutual information between tuple and coordinate embeddings (using CLUB estimator, optimized with gradient descent).
   - Symmetry group label is available to both discrete and continuous stages and during augmentation.
3. **Losses & Deduplication**
   - Cross-entropy for tuple prediction, MSE for coordinates, augmented with permutation- and orbit-invariant matching (Hungarian/Monte Carlo for orbits; fallback to MC for >12 Wyckoff positions/group).
   - Auxiliary property-prediction loss: property surrogate (CGCNN, refit on MP-2022 for generated data distribution).
   - Symmetry-aware uniqueness loss: penalizes latent collisions after canonicalization.
4. **Filtering & Evaluation**
   - Prior to DFT validation, rank by surrogate (CGCNN) stability. DFT workflow (VASP, PBE, PAW, 400 eV cutoff, 1e-5 eV convergence) is run on the top 5% by surrogate score, with explicit resource tracking (e.g., 8 A100 GPUs; 10k samples ~48 hours training).
   - Physical plausibility filters applied (max bond: 2.8 Å; min cell angle: 80°) before DFT.
   - All generated outputs deduplicated via canonicalization scheme for final novelty assessment.
5. **Benchmarking & Scaling**
   - Initial experiments focus on space groups 1–10 (sample size: >200/group); threshold for orbit matching is set to K=12 per group, with MC fallback for larger groups.
   - All hyperparameters, code, and data splits detailed for reproducibility.

This protocol directly distinguishes itself from FlowMM, DiffCSP, and WyCryst by (i) using a joint, canonical and group-conditioned encoding, (ii) deduplication/orbit-invariant losses at both discrete and continuous stages, and (iii) detailed resource-constrained scaling plans.


Hypothesis ID: 2
Averaged Score: 4.5; Scores: [5, 4, 4, 5]
Number of rounds: 3
We hypothesize that: An explicit, two-stream generative model, combining (1) a legality-masked, autoregressive transformer for sequential prediction of canonical Wyckoff-site/species assignments, and (2) an SE(3)-equivariant GNN conditioned on space group and Wyckoff context for asymmetric unit coordinate generation, each grounded in a formally canonical (lex-min) Wyckoff encoding, can faithfully reproduce materials space group distributions and outperform prior models in stability, uniqueness, and novelty.

Key protocol components:
1. **Data & Representation**:
   - Datasets: MP-20, ICSD, OQMD, filtered for ≥95% unambiguous symmetry assignment (see pseudocode repo).
   - Canonicalization: Lex-minimization over Wyckoff representations with permutation pruning; explicit handling for ambiguous/multi-label cases, with algorithm/test suite open-sourced.

2. **Model Architecture**:
   - Stream 1: Transformer (4–8 layers, 256–512 dim, 8–12 heads) predicts Wyckoff-site+species sequence, legality-masked per (space group, occupancy), autoregressive decoding; masking computed via cached precomputed group tables, fallback to on-the-fly spglib check for large groups.
   - Stream 2: SE(3)-equivariant GNN (e.g., e3nn, 3–6 layers) outputs asymmetric unit coordinates and orbital attribute vectors, conditioned on space group and Wyckoff tuple.
   - Group-conditioning shared by both streams; orbit-to-orbit consistency encoded as input feature.

3. **Orbit-Invariant Regularizer**:
   - Loss term: L_orbit = E_g [ D(f(x), f(g⋅x)) ], where f is site encoding, g in group G; implemented batchwise over all group elements; cannot be replicated by augmentation alone (see proof in supplement).
   - Additional: Mutual Information loss (CLUB or variational lower bound) to enforce Wyckoff-species-embedding dependence.

4. **Training & Hyperparameters**:
   - Joint objective: λ1*cross-entropy (discrete) + λ2*SE(3)-invariant coord loss + λ3*L_orbit + λ4*MI-loss; λ1–λ4 searched on [0.1,1], grid released in YAML config.
   - Fallback sampling: up to 1024 attempts; if all fail (illegal sequence), abort/log/sample rejection, stratified per group.

5. **Evaluation**:
   - Baselines: CrystalFormer, WyckoffGAN/Set, SOTA diffusion (official configs), all retrained and scored on released splits.
   - Metrics: space group fidelity, physical stability, uniqueness (structural/chemical), DFT funnel pass-rate, resource/runtime benchmarks.

6. **Reproducibility**:
   - All codebases, preprocessing, canonicalization, configs, and model checkpoints pre-published; ambiguous-case logs/decisions archived.

7. **Broader Impact**:
   - Protocol generalizes to e.g., magnetic/disordered/alloy materials (public challenge/benchmark).

This canonical, symmetry-regularized, group-invariant two-stream design is precisely specified for transparent reproducibility, thorough downstream evaluation, and unambiguous positioning vs. prior art.


Hypothesis ID: 3
Averaged Score: 4.25; Scores: [4, 4, 4, 5]
Number of rounds: 2
We hypothesize that a fully reproducible generative pipeline for crystal structures can be achieved by integrating (1) open-source canonicalization of Wyckoff encodings using a lex-order, frame-anchored convention (with tunable tolerance and permutation pruning), (2) explicit equivalence-class augmentation capped and sampled to ensure tractability even for high-symmetry groups, and (3) an autoregressive transformer with legality-masked sequence modeling, in which legality masks are both computed by canonical group-theoretic rules and refined as learnable auxiliary outputs via cross-entropy loss and soft-oracle supervision. 

The model employs modular, group-adaptive transformer heads, defined by a Wasserstein distance metric over site multiplicities, sharing parameters across similar groups and reverting to pooled adapters for rare/undersampled cases. During training and inference, space group stratification and augmentation ensure balanced representation, with strict prevention of group and permutation hash leakage. The model is optimized via combined sequence, space group distribution, uniqueness (set-invariant), and legality mask loss terms. Property-driven screening employs both DFT and ML-based proxies, with all scriptable pre/post-processing made publicly available for replication.

Ablation protocols for augmentation, legality mask strictness/learnability, and canonicalization thresholds are prescribed; scaling fallback and surrogate strategies for combinatorial high-symmetry situations are included. All key artifacts (code, configs, hashes, legal mask computation, canonicalization, augmentation) are published for full reproducibility. This approach enables generative models that faithfully reproduce the natural distribution of crystal symmetries with high stability, uniqueness, and novelty, constituting a significant methodological advance for AI-driven materials discovery.


Hypothesis ID: 4
Averaged Score: 4.25; Scores: [4, 4, 4, 5]
Number of rounds: 2
We hypothesize that a hybrid generative model—integrating autoregressive space group-conditioned Transformer blocks with permutation- and group-equivariant set modules—trained on stratified and legality-augmented Wyckoff-based encodings, will enable faithful reproduction of the empirical distribution of symmetry space groups, while achieving high stability, uniqueness, and novelty in generated crystals. The methodology comprises:

1. Data Encoding: Encode each crystal as a sequence of Wyckoff positions, atom species, and site-specific features. Generate all symmetry-equivalent Wyckoff representations; in each batch, sample a capped stratified subset for augmentation. Embed space group as both a one-hot vector and learned group embedding. Prepare legality masks per sequence position using cached, precomputed or runtime group operations; if all possibilities are illegal, resample from alternate augmentations (soft penalization as fallback).

2. Model Architecture:
- Alternating Blocks: Alternate k-layer stacks of autoregressive Transformer (causal masked multi-head self-attention, e.g., 4–8 layers, 256–512 heads/units) and set-equivariant modules (e3nn or similar, l={0,1,2} channels, hybrid batch-dot with site and group channels; 2–4 blocks per pass). After each block group, concatenate and linearly project sequence and set outputs to a unified latent, passing residuals to both streams.
- Cross-Talk: Use cross-attention and pooled concatenation to share information from sequence to set module and vice versa every N blocks (N tunable). Dimensional mismatches handled via projection and normalization. Fusion hyperparameters fixed by ablation (e.g., N ∈ {1,2,3}).

3. Training:
- Objective: Hybrid loss combining (a) autoregressive categorical cross-entropy (CCE) over sequences, (b) Hungarian-matched orbit-invariant set CCE, (c) group-equivariant geometric RMSD, with starting weights λ_seq=0.5, λ_set=0.3, λ_geom=0.2 (search over ±0.1 per ablation). Use AdamW optimizer (lr=5e-4), batch size ≈64, 1:1 augmentation of sampled equivalence classes, legal mask dropout at 10% for robustness.
- Legality Masking: Hard rejection with immediate resample; if needed, soft negative log-likelihood penalty for unavoidable illegal options.
- Ablation: Training runs include (i) Transformer-only, (ii) set-module only, (iii) hybrid with/without augmentation, (iv) with/without legality masking. Evaluate per space group, uniqueness, stability (DFT/ML proxy), and RMSD to ground truth.

4. Example Pipeline: For each crystal, enumerate up to 16 symmetry-equivalent Wyckoff orderings. Batch sample 2–4 each, align via legality masks, and process through alternating model blocks. Output both canonicalized sequence and set representations. Example augmentations and legality maps provided as supplementary data for reproducibility. All hyperparameters and e3nn settings logged and open-sourced.

This protocol addresses sequence/set ambiguity, robust legality, and symmetry fidelity, within resource-constrained and reproducible limits.


Hypothesis ID: 5
Averaged Score: 4.25; Scores: [4, 4, 4, 5]
Number of rounds: 3
We propose a reproducible, canonicalizable generative modeling framework for crystalline materials that explicitly encodes and generates Wyckoff-based symmetry group distributions, integrating (a) modular group-adaptive transformer architectures with legality-masked autoregressive sequence modeling, (b) a latent decomposition of asymmetric units and corresponding symmetry transformations, and (c) rigorous uncertainty handling, fallback protocols, and detailed mechanistic diagnostics.

Data Representation & Canonicalization:
- Each crystal is encoded by (i) ordered Wyckoff position sequence (referenced to group and multiplicity), (ii) atomic species, and (iii) explicit symmetry transformation parameters for each unique orbit. Wyckoff sequence and orbit assignment are canonicalized via lexicographic convention, spglib-based frame anchoring, and permutation-invariant hashing (SHA256), with tolerance parameters (e.g., 0.05–0.1 Å; tuned via grid search on structural equivalence recovery) justified by maximizing in-dataset uniqueness while preserving physical plausibility.

- For high-multiplicity/ambiguous representations, exhaustive enumeration is capped (tunable cap, default 25), with compact random sub-sampling and augmented batches to cover equivalence classes. Diagnostic scripts audit class coverage and hash collisions.

Model Architecture:
- The generation pipeline is an autoregressive transformer stack with jointly trained, parallel branches:
  (1) Wyckoff Symbol/Multiplicity Sequence (categorical, legality-masked at each step; legality imposed by (i) hard canonical rules, (ii) learnable mask head regularized on known-legal/illegal sequences, and (iii) adjustable soft gating—sigmoid-masked attention weight);
  (2) Atomic Species Prediction (shared with Wyckoff prediction via multi-tasking heads);
  (3) Latent Block: Directly predicts interpretable symmetry transformation parameters (rotation matrices, translation vectors) for each asymmetric site. This block is trained with a hybrid loss: (i) orbit-level permutation matching (Hungarian algorithm, RMSD threshold 0.2 Å with uncertainty flagging), (ii) explicit orbit-invariant/variant reconstruction, and (iii) novel disorder regularization schedule enabling controlled symmetry breaking.

- Transformer heads are modularly grouped by clusterings on group multiplicity vectors and geometric descriptors (e.g., mean orbital spans), clustered with cosine-similarity linkage; cluster assignments are published with dendrogram visualizations and can be manually audited.

Training & Evaluation:
- Stratified sampling ensures all space groups and permutation hashes are in exclusive train/test sets; rare groups (<10 structures) invoke adaptive head-pooling (nearest-cluster fallback), and disorder-prone groups invoke uncertainty-weighted loss. Quality checks trigger human-in-the-loop or synthetic augmentation if ambiguity flags exceed set thresholds.

- Legality, uniqueness, and symmetry distribution fidelity are penalized via composite loss: (i) categorical cross-entropy, (ii) group presence EMD penalty, (iii) set-invariant uniqueness loss, (iv) legality mask accuracy, (v) latent transformation/orbit regularization with explicit uncertainty weighting.

- Property-guided screening invokes pooled ML-based prefiltering followed by DFT (VASP, fixed cutoffs), with automated error reporting and hash-logged input/output data.

- All pipeline elements—canonicalization config, model code, parameter search script, diagnostics for mask dynamics, head assignment, and latent visualization—are published open-source, with full data, hashes, and documentation to assure reproducible deployment and benchmarking.


Hypothesis ID: 6
Averaged Score: 4.25; Scores: [4, 4, 4, 5]
Number of rounds: 1
We propose a generative model for inorganic crystals that combines explicit Wyckoff position canonicalization with a modular, group-conditional masked transformer architecture and symmetry-aware metric losses, aiming to faithfully reproduce symmetry group distributions and overcome representational ambiguities observed in SOTA methods.

1. Data Representation and Canonicalization:
- Crystal structures are encoded using the minimal set of Wyckoff positions and site-specific attributes (atomic type, occupancy, fractional coordinates).
- Canonicalization is performed per structure by: (i) identifying all symmetry operations (via spglib), (ii) generating all equivalent Wyckoff letter sequences (where site permutations are restricted by atomic type and occupancy), (iii) selecting the lexicographically minimal encoding as the canonical representative. Partial occupancies/disorder are handled by introducing special tokens and by extending permutation rules to tolerate mixed sites.
- Algorithmic steps for canonicalization are explicitly defined (see Algorithm 1 below), and computational complexity is controlled via pruning (e.g., stopping once a canonical candidate is found, or restricting permutations with high site multiplicity). Performance benchmarks are reported as average/worst-case over the training dataset.

2. Model Architecture:
- The backbone is a masked transformer encoder with standard multi-head self-attention and token/positional embeddings for group, Wyckoff labels, atomic type, occupancy, and coordinates.
- Group-conditional heads: Each crystallographic space group either receives a dedicated output head (if sample count exceeds threshold N) or shares a head with similar groups using parameter sharing and group embedding vectors. Cross-group sharing is guided by group-theoretic similarity.
- Decoder heads generate atomic types, Wyckoff sites, and coordinates in sequence. For disorder, uncertainty-aware regression heads are used.
- Heads are conditioned on the full latent sequence up to the generation point, and on group embeddings. Parameter sharing minimizes overfitting for rare groups.

3. Training and Losses:
- Main generative loss: Negative log-likelihood of properly canonicalized Wyckoff token sequence.
- Symmetry-uniqueness loss: For each minibatch, penalize generation of symmetry-equivalent outputs (identified via canonicalization and structure hashing: L_uniqueness = λ∑_i,j δ(can(X_i) = can(X_j))), efficiently computed using minibatch hashes.
- Symmetry-group distribution alignment: Minimize Earth Mover’s Distance (EMD) between generated and training set distributions of space groups (L_group).
- Optional auxiliary losses: Stability surrogate (fast property prediction), coordinate regression (for partial occupancies/disorder), and feasibility filters.

4. Benchmarking and Validation:
- Experiments performed on the latest MP-20 and COD datasets, using consistent train/val/test splits and pre-processing as SOTA baselines (e.g., DiffCSP, WyCryst, FlowMM).
- Evaluation metrics include validity (symmetry, connectivity, Wyckoff correctness), uniqueness (fraction of non-duplicate structures), novelty (fraction not in training set), group distribution (EMD), structural stability (using surrogate models/DFT screening), and runtime (average/worst-case canonicalization time, training speed).
- Ablation studies further test the contribution of canonicalization, group-specific heads, and each loss component; runtime/complexity profiles are reported.

Algorithm 1 (Wyckoff Sequence Canonicalization):
Input: Structure S; Output: Canonical Wyckoff token sequence
1. Enumerate all symmetry operations for S’s space group (spglib)
2. Generate all Wyckoff label assignments consistent with atomic types/occupancies
3. For each, build sequence; pick lex-minimal as canonical representative
4. Return canonical sequence and structure hash

All implementation specifics, loss equations, and benchmarking scripts are published for reproducibility and direct comparison with prior works.


Hypothesis ID: 7
Averaged Score: 4.25; Scores: [4, 4, 4, 5]
Number of rounds: 1
We hypothesize that a generative model explicitly operating in the quotient space of atomic positions and Wyckoff assignments modulo space group actions—the symmetry-orbit space—will more faithfully reproduce the symmetry distribution and diversity observed in real inorganic crystals. This approach will use the following methodology:

1. **Data Representation**
   - Each crystal is encoded as (P, W, G): fractional atomic positions (P), Wyckoff position assignments (W), and space group label (G).
   - Multiple equivalent representations of (P, W) for the same crystal (from group actions or Wyckoff relabeling) are treated as belonging to the same equivalence class. These are enumerated using symmetry operation tables from standard crystallographic libraries (e.g., spglib).
   - For dataset construction, we use MP-20 or similar, including atomic identities, space group, and explicit Wyckoff sites; data is augmented via group actions.

2. **Physical Plausibility Filtering**
   - Structures with any pairwise atomic distance < 0.8 Å (computed under periodic boundaries) are immediately rejected. Further filters (e.g., minimum bond valence sum deviation or void detection) may be applied optionally at generation or post-processing stages.

3. **Model Architecture**
   - **Encoder:** A group-equivariant graph neural network (SE(3)-equivariant or, for higher accuracy, explicitly tailored to the space group G) with 3–6 layers and 64–256 hidden features processes (P, W, G) into an invariant latent representation.
   - **Decoder:** Either an autoregressive Transformer or diffusion model (with 4–12 layers), operating over indices in the symmetry-orbit space rather than a canonical Wyckoff labeling. Symmetry group G is incorporated as a conditioning variable via learned embeddings.

4. **Orbit-Invariant Loss Function**
   - For each generated structure, orbit-invariant comparison to target is performed: all group-equivalent (P', W') forms are generated, canonical representatives are sampled, and pairwise root-mean-square distances (RMSD) (or feature-space similarity) are minimized using the Hungarian algorithm for label/permutation ambiguity, constrained to symmetry-allowed mappings.
   - The loss averages over a stochastic sample of orbit representatives if full enumeration is infeasible (scaling: O(N_atoms × |G|)).

5. **Training and Computational Protocol**
   - Datasets are loaded in batches, to fit within ≤32 GB GPU memory for standard sets (~5,000–50,000 structures). Symmetry group orbits and mapping tables are cached, or sampled Monte Carlo-style for groups with |G| > 192. Training is run for 100–200 epochs, optimizing orbit-invariant loss.

6. **Decoding and Ambiguity Resolution**
   - Generated samples are mapped from latent space back to explicit (P, W, G) using canonicalization (with random tie breaking). Redundant or ambiguous Wyckoff assignments are collapsed using a deterministic heuristic (e.g., site multiplicity ordering).

7. **Validation and Assessment**
   - Physical plausibility is verified as above. For select candidate structures, ab initio (DFT) computations or empirical stability indicators are used for post hoc validation.

8. **Contrast with Prior Work**
   - Unlike canonicalization-based GNNs (e.g., SchNet, PGCGM) or E(3)-equivariant models, this method directly models within symmetry orbits and naturally handles non-unique Wyckoff representations, providing stronger symmetry faithfulness without output-only projection. To our knowledge, no prior generative model in crystallography or molecular chemistry integrates latent space group-action equivariance in this way.


Hypothesis ID: 8
Averaged Score: 4.0; Scores: [4, 4, 3, 5]
Number of rounds: 1
Hypothesis: 

A symmetry-aware generative model for crystalline materials, which (1) represents crystal structures as sets of Wyckoff positions grouped by their space group equivalence classes, (2) employs explicit equivalence-class augmentation and dynamically legality-masked autoregressive generation, and (3) is trained with set/permutation-invariant losses, will better reflect the true distribution of symmetry groups and improve stability, uniqueness, and novelty of generated crystals compared to existing Wyckoff-based and standard baseline models.

**Methodology Details:**

**1. Data Representation:**
- Each crystal is encoded as a set of (space group number, Wyckoff site letter, atomic species, fractional coordinates). For training, enumerate all symmetry-equivalent representations via spglib/pymatgen. If the number exceeds N_max (e.g., N_max=32), select a random but stratified subset per sample with a fixed random seed.

**2. Model Architecture:**
- Transformer decoder, 8 layers, 8 attention heads, hidden dim 512. Input sequences padded/packed to uniform length, with learned positional embeddings and atom-type embeddings. Legality masks (computed via pre-tabulated constraints from spglib) applied at each generation step to forbid symmetry-infeasible moves; these are precomputed for common space groups and dynamically constructed beyond a frequency threshold. The model is trained via Adam optimizer (lr=1e-4, weight_decay=1e-2), batch size 128.

**3. Training Objective:**
- Loss function is permutation-invariant (DeepSets sum or mean aggregation of sequence-level likelihoods, plus a contrastive loss between generated and canonical representations). When equivalence-class augmentation is used, random augmentation is applied per epoch, and augmented samples are shuffled across batches. Training, validation, and test splits are 80/10/10%, held fixed with a designated random seed.

**4. Edge Cases and Failure Handling:**
- If a legality mask blocks all next-token choices, generation is terminated for that structure (early stopping), and the sequence is marked as invalid. Rejection sampling is used up to 5 times to recover a legal path per generation.

**5. Screening/Benchmarking:**
- After model generation, structures are canonicalized and checked for uniqueness using symmetry-invariant graph hash functions. Representative structures per test batch are prioritized for DFT-based (e.g., VASP, encut=520 eV, kpoint mesh ≥3x3x3, energy cutoff 0.05 eV/atom) or ML potential-driven stability screening, prioritizing those with rare space groups and novel compositions. Segregate screening sets for full ablation. Output is compared with SOTA WyckoffSet, CrystallGAN, and baseline Transformer using identical splits/metrics.

**6. Scalability Measures:**
- For space groups with very high equivalence class sizes, batch-level parallel mask computation is used. Augmentation factor N_max is tuned via validation performance. The subset of generated structures for first-principles screening is kept below 10% per epoch for resource management.

**Prior Work Distinction:**
- No prior generative protocol simultaneously integrates full equivalence-class augmentation, legality-masked autoregressive sequence generation, and permutation-invariant loss, making this a methodologically novel approach in symmetry-driven generative materials design.


Hypothesis ID: 9
Averaged Score: 4.0; Scores: [4, 4, 3, 5]
Number of rounds: 1
Hypothesis: A modular generative framework, based on explicit Wyckoff position combinatorics and symmetry constraints, with graph-based representations and structured loss functions, will more faithfully reproduce symmetry group distributions and structure uniqueness in inorganic crystals compared to state-of-the-art models.

1. **Data Representation**
    - Each crystal structure is encoded as a bipartite graph: one set of typed nodes represents distinct Wyckoff positions (uniquely labeled per space group, including multiplicity and site symmetry labels); the other set represents chemical species assigned to each site.
    - Node features: For Wyckoff nodes—position type, multiplicity, site degree of freedom flags, and any site-specific parameterizations (e.g., continuous free coordinates). For chemical species nodes—elemental identity (e.g., atomic number, Pauling electronegativity, ionic radius), occupancy.
    - Edge features encode assignment relations; graph connectivity follows valid Wyckoff-group–element assignments per structure.

2. **Dataset and Scope**
    - Training dataset is derived from the Materials Project and ICSD, filtered to exclude structures with partial occupancies or disorder; include all 230 space groups (with data balancing to mitigate over-dominance by common groups).
    - Approximately 50,000–200,000 unique stoichiometric compounds, stratified to maximize Wyckoff/orbit diversity.
    - Separate train/validation/test splits are established, ensuring unseen Wyckoff combinations in the test set where feasible.

3. **Generative Model Architecture**
    - Core: Conditional GraphVAE with:
        - Encoder: 3-layer Message Passing Neural Network (MPNN; 128 hidden units/layer, ReLU activation), reading bipartite graphs.
        - Latent: Gaussian-distributed, 128d latent code.
        - Decoder: Conditional normalizing flow, with GatedGCN backbone (3 layers, 128 hidden), generating node/edge attributes and site-specific continuous parameters.
        - Conditional input: target space group ID (one-hot, 230d), optional chemical composition vector.
        - Discrete sampling: Wyckoff letters sampled using Gumbel-Softmax relaxation; chemical elements sampled using conditional masked softmax.

    - **Losses:**
        1. Reconstruction (graph structure, node features, continuous Wyckoff parameters)
        2. KL divergence (VAE regularization, β=1.0)
        3. Symmetry invariance: orbit-wise hash code consistency (using crystallographic orbit hash), weighted at λ_sym=0.2
        4. Novelty encouragement: penalize close neighbors in latent/orbit-hash space (margin loss, λ_nov=0.1)
        5. Auxiliary: Smoothness penalty on decoded positions (λ_smooth=0.05)

    - **Hyperparameters:** batch size = 128, learning rate = 1e-3 (Adam); 100 epochs minimum or to convergence.
    - Models implemented in PyTorch Geometric with custom crystallography modules; symmetry validation with spglib.

4. **Sampling and Conditioning**
    - Generation can proceed both unconditionally and conditioned on space group, composition, or partial Wyckoff configuration.
    - Combinatorially valid Wyckoff orbits are filtered using chemical/structural priors:
        - Element–site compatibility (ionic radii, charge-balance, empirical bonding)
        - Exclusion of physically unreasonable orbits (e.g., overlapping atoms)
        - Negative sampling/penalization on invalid configurations during training

5. **Validation and Evaluation**
    - Symmetry retention: assigned via spglib with >0.95 similarity threshold to nominal space group
    - Uniqueness: Orbit hash codes; explicit check for duplicates over the test/generation set
    - Stability: For a random 1% subset, DFT relaxations using VASP (PBE functional, 520 eV cutoff, k-point mesh ≤0.25 Å⁻¹)
    - Additional: Evaluate map coverage in the (space group, Wyckoff) combinatorial space; track chemical/structural plausibility rates.

6. **Novelty vs. Prior Art**
    - Compared to preceding equivariant or point-group-aware models, this framework uniquely models the combinatorial generation and explicit invariance of Wyckoff orbits at the generative level; no prior work combines graph VAE, conditional flows, and orbit-hash regularization as described.
    - Provide a comparative table in publication to demonstrate this.


Hypothesis ID: 10
Averaged Score: 4.0; Scores: [4, 4, 3, 5]
Number of rounds: 1
We hypothesize that a modular, symmetry-aware generative model can faithfully learn and reproduce the distribution of inorganic crystal structures by sequentially constructing space group, subgroup path, and Wyckoff positions, while explicitly managing symmetry-equivalence. The model is defined as follows:

1. **Representation:** Each crystal is represented by (a) a starting space group G (from standard tables), (b) a path P = (G → H_1 → ... → 1) traversing the subgroup lattice, with one descent per step, (c) at each node, a set of Wyckoff sites (discrete, categorical), (d) for each site, associated continuous positional parameters and (optional) orientational degrees of freedom, (e) a mapping of chemical species to sites, and (f) additional constraints such as charge neutrality.

2. **Generative Process:**
  a. The top-level model samples space group G and, conditional on G, learns a probability distribution over permissible subgroup paths P using an autoregressive transformer (with a masking mechanism to eliminate forbidden or trivial paths, and learned probabilities to match observed dataset path frequencies).
  b. For each node in P, the model samples occupied Wyckoff sites via a categorical distribution parameterized by the context (site symmetries, path history, chemical context). Discrete Wyckoff letters are sampled sequentially, with site splitting allowed as per group theory rules.
  c. For each occupied site, continuous Wyckoff parameters are sampled via a conditional normalizing flow (e.g., Masked Autoregressive Flow), conditioned on group, site symmetry, and chemical context. Discretization is used for low-symmetry special positions, with a fixed number- of bins (default 32 per axis, adjustable).
  d. Chemical species assignment per site is generated with masking for allowed valence and site multiplicity, and hard-masked for overall charge neutrality at each step.

3. **Symmetry-Equivalence Management:** Canonicalization is applied after each generative step using Spglib to map the current configuration to a representative canonical form. For efficiency, a local cache is maintained for duplicate configurations, and batch hash-mapping is employed for large scale sampling. Site splitting and merging are equivalence-checked using site permutation graphs.

4. **Model Training:**
  a. Training data is derived from Materials Project and OQMD, with group-subgroup paths computed via symmetry analysis. Ambiguous or missing paths are imputed by selecting the minimal descent chain according to group order.
  b. The full model is trained end-to-end to maximize likelihood of observed (G, P, Wyckoff, species, parameters) tuples, with auxiliary supervised objectives: (i) predictive losses for stability (ML property predictor), (ii) uniqueness losses (contrastive on canonically labeled graphs), (iii) charge neutrality constraints.

5. **Implementation Defaults:**
  - Transformer blocks: depth=4, width=256, Alibi positional encoding.
  - Normalizing flows: 3 layers/site.
  - Discretization bins: 32 (can be varied for memory/performance).
  - Libraries: Pymatgen/Spglib for symmetry, PyTorch for modeling.

6. **Benchmarking and Validation:**
  - Generation compared to SOTA models (DiffCSP, FlowMM) with metrics: symmetry fidelity (% correct SG), diversity, uniqueness, stability (ML-predicted), and novelty (cross-referenced with database).
  - Ablations include (i) path-sampling autoregression vs uniform, (ii) masking vs post-hoc filtering, (iii) no canonicalization.

Fallback: For rare or ambiguous group paths, the model defaults to shortest observed chains and major Wyckoff configurations. Non-standard or modulated structures are outside scope but may be handled in future extensions.


Hypothesis ID: 11
Averaged Score: 4.0; Scores: [4, 4, 4, 4]
Number of rounds: 1
We hypothesize that a generative model for inorganic crystal structures integrating explicit Wyckoff position encoding, modular space-group-specialized transformer architectures, and real-time canonicalization with symmetry-aware uniqueness penalization will faithfully reproduce the distribution of symmetry space groups as in empirical datasets, while surpassing previous models in stability, uniqueness, and novelty. The model comprises:

1. **Data Representation and Filtering:**
   - Crystals are represented as triples (space group, ordered Wyckoff positions per site, associated chemical species and occupancy).
   - All entries with partial occupancy/disorder are excluded using a formal protocol (e.g., pymatgen site occupancy < 1.0 or flagged disorder fields), provided as an open-source filter script. Maximal site count per space group is determined from the dataset, with fixed-length encoding (assignment or masking) for the transformer input; leftover lengths are zeroed and masked.

2. **Canonicalization In-Loop:**
   - At each generative step, the produced structure is canonicalized in real-time via spglib/pymatgen, enforcing a unique lex-minimized Wyckoff/path/site ordering. A category-preserving hash is computed (e.g., via standardized Wyckoff+species+space group tuple hashes), with a global batch cache to prevent duplicates; algorithmic pseudocode and code are provided for community use. Failure to canonicalize triggers rejection or resampling as per a defined fallback protocol.

3. **Model Architecture:**
   - The generator is a modular transformer with a shared backbone and group-specific output heads, parameterized for the number of SGs (e.g., N_layers = 8, embed_dim = 512). Wyckoff positions are modeled as categorical variables, while continuous positional/chemical parameters are generated via conditional normalizing flows, all conditioned on space group, site context, and chemical composition. Site-species assignments are masked using tabulated, open-source compatibility rules; charge neutrality and chemical validity are enforced at each step through constraint-based filtering.

4. **Training Objective:**
   - The total loss L = L_data + λ_unq L_uniqueness + λ_sg L_dist includes: (i) a data likelihood loss over the discrete (Wyckoff) and continuous (position, composition) components, (ii) a mathematically-formulated uniqueness penalization based on batch-and-cache hashes (provided in appendix), and (iii) an Earth Mover’s Distance-based symmetry group distribution penalty, all precisely defined.

5. **Evaluation & Ablation:**
   - Models are benchmarked on train/val/test splits from MP-20 and extended databases, using fixed random seeds; DFT-based surrogate models screen for stability (uncertainty threshold pre-defined), and fingerprint-based novelty detection pipelines are described and released. Ablation toggles canonicalization (real-time/post hoc/none), uniqueness loss, and modularization. Throughput/runtime is reported. Code, hashes, filters, and benchmarking scripts are provided for community standards.

Thus, this framework sets a reproducible, openly validated, and extensible standard for symmetry-respecting, uniqueness-preserving crystal generation, ready for direct adoption and benchmarking by the materials design community.


Hypothesis ID: 12
Averaged Score: 4.0; Scores: [4, 4, 3, 5]
Number of rounds: 1
We hypothesize that a generative model for inorganic crystal structures based on a strictly canonical, symmetry-invariant Wyckoff representation, trained via a conditional diffusion probabilistic model (DDPM) with a transformer backbone, will faithfully recapitulate the symmetry group distribution of real crystals, while surpassing prior Wyckoff-based and state-of-the-art (SOTA) methods in stability, uniqueness, and novelty of generated structures. The methodological steps are:

1. **Data Preparation & Representation:**
   - Select only non-disordered, fully occupied inorganic materials with standard-space-group settings from MP-20.
   - Canonicalize each structure's Wyckoff representation in two stages: (a) use spglib to identify the minimal standard setting and derive all symmetry-equivalent Wyckoff encodings; (b) select the global canonical ordering by lexicographically ranking all stringified Wyckoff configurations, using explicit tie-breaking and parity rules to resolve enantiomorphic and degenerate cases.
   - Encode Wyckoff data as hybrid embeddings: (i) Wyckoff letters as categorical tokens (embedding size=32, padding for variable sites), (ii) atomic types (embedding size=32), and (iii) site coordinates or positional invariants (continuous, normalized to [0,1]).

2. **Data Augmentation:**
   - Augment via minor fractional translations and systematic element permutations within symmetry-allowed limits, always re-canonicalizing outputs to enforce a single unique encoding per crystal.
   - Explicitly exclude rare/ambiguous settings, logging exclusion rates and reasons.

3. **Model Architecture:**
   - Implement a conditional DDPM with a transformer backbone: 8 layers, 8 attention heads, embedding size 256. Condition on chemical composition and canonical space group labels.
   - Diffusion schedule: 1000 linear timesteps, beta-min=1e-4, beta-max=0.02.
   - All model outputs are mapped to valid, canonicalized Wyckoff representations, with an invertible-to-CIF postprocessing layer using spglib and custom parsing routines.

4. **Training Objective:**
   - Minimize a composite loss: (i) Wyckoff and atomic type categorical cross-entropy, (ii) coordinate reconstruction (MSE or Huber), (iii) explicit group-theoretical penalty for invalid/collapsed symmetries (magnitude=2.0), (iv) uniqueness-promoting regularizer (triplet loss on embedding, margin=0.2). Hyperparameters are validated via grid search.

5. **Validation & Edge-Case Handling:**
   - All outputs are checked for invertibility (CIF recovery), spatial overlap, symmetry mislabeling, and physical plausibility (lattice parameters/atomic separations). Non-invertible or ambiguous cases are logged/fixed via fallback routines (e.g., majority rule assignment for degenerate Wyckoff labelings).
   - A unit test suite validates correct canonicalization on all known edge cases from spglib and custom-compiled rare symmetry examples.

6. **Evaluation Protocol:**
   - Evaluate sample sets using: (i) KL divergence for symmetry group distribution vs. reference, (ii) uniqueness rate after canonicalization, (iii) symmetry violation rate, and (iv) proxy stability via pretrained graph neural network surrogate. All metrics are computed on held-out test splits and ablation subsets.

By systematically enforcing canonical, invertible symmetry-aware Wyckoff encodings at every cycle, this hypothesis posits a robust, reproducible advance in generative crystal science.


Hypothesis ID: 13
Averaged Score: 4.0; Scores: [4, 4, 4, 4]
Number of rounds: 1
Hypothesis: A group-equivariant set generative model, which explicitly marginalizes over all symmetry-equivalent Wyckoff site assignments (as sets) for a crystal, will more faithfully reproduce the real-world distribution of symmetry space groups and generate more stable, unique, and novel crystal candidates than state-of-the-art models.

Methodology:

1. **Data Representation and Preprocessing:**
    - Each crystal is represented by (i) its Wyckoff position assignments (categorical per atom, using international table labels), (ii) space group index, and (iii) minimal asymmetric unit coordinates.
    - For each training instance, enumerate all symmetry-equivalent Wyckoff representations using spglib or PyXtal, up to a tunable max-per-structure N (e.g., N=64; chosen by balancing dataset statistics, point group order, and GPU memory limits). If enumeration is infeasible, perform uniform random sampling over the group orbit while ensuring coverage and diversity (using stratified or rejection sampling based on equivalence class frequency).
    - Exclude ambiguous/degenerate cases via symmetry checks, or treat as edge-case ‘validation exceptions’.

2. **Model Architecture:**
    - Base model is a permutation/set-equivariant neural network (DeepSets or Set Transformer backbone) with explicit group-equivariant layers; for space group handling, employ induced action matrices as permutation transformations over site assignments. Embedding sizes, attention heads, and depth (e.g., 128-dim/feature, 8-layer) are set via grid search, detailed in a hyperparameter table.
    - For handling physical/geometric constraints, concatenate symmetrized fractional coordinates and Wyckoff site embeddings, applying geometric feature augmentations (e.g., pairwise distance histograms as auxiliary channels).
    - Input/Output shapes: Fixed max-unrolled Wyckoff set length (pad/truncate as needed), with set output interpreted as a discrete distribution over possible Wyckoff site sets per space group.

3. **Loss Function and Training Procedure:**
    - Employ a differentiable set-to-set matching loss, combining a soft assignment Sinkhorn loss between predicted/generated Wyckoff sets and all enumerated true equivalence-class representatives; negative samples are drawn both from other space groups’ Wyckoff sets and physically invalid assignments (filtered via geometric/charge neutrality rules).
    - Marginalization is implemented by averaging (min aggregation as ablation) over all valid equivalence-class representatives per training datum. For intractable large sets, sample a bounded number of equivalence representatives, always including the canonical form.
    - Batch training leverages augmentation by shuffling outbound equivariant representatives and includes ablation batches omitting random group actions for robustness.

4. **Physicality and Symmetry Validation:**
    - Model outputs are filtered by: (i) fast, hard-coded rule-based checks (minimum interatomic distance >1 Å, no unphysical overlaps, overall charge neutrality), (ii) rapid classical forcefield scoring (e.g., via OpenBabel or GULP with single-point relaxation), and (iii) optional third-stage DFT validation (offline postprocessing, not during training).

5. **Baselines and Evaluation:**
    - Explicitly benchmark against DiffCSP, FlowMM, prior Wyckoff-based models, and symmetry-canonicalization baselines; all metrics (uniqueness, symmetry-faithfulness, novelty, stability) follow MatBench/Materials Project protocols, with thresholds (e.g., at least 95% symmetry-match to database) specified in the experimental appendix.
    - Report reproducibility settings: public datasets used, code repository with all hyperparameters, training logs, and a Docker image specifying minimum 32GB GPU memory for reference workloads.

6. **Novelty Clarification:**
    - The principal advance is set-valued output over full symmetry equivalence classes, marginalized at training and inference, contrasting prior single-canonical or invariant approaches. Added literature table in supplementary for explicit comparison and gap analysis.


Hypothesis ID: 14
Averaged Score: 4.0; Scores: [4, 4, 3, 5]
Number of rounds: 1
A generative modeling framework that explicitly respects crystallographic symmetry via orbit-marginalized set-valued Wyckoff representations, robust canonicalization and ambiguity management, and group-equivariant neural architecture, will reproducibly and faithfully generate crystal structures matching real-world symmetry distributions. The following hypothesis details the essential methodological design.

1. Data Representation & Preparation:
- Each structure is formatted as: (a) integer space group label; (b) complete set of Wyckoff positions (categorical, per-site); (c) asymmetric unit coordinates; (d) full group operations matrix (from spglib/PyXtal).
- For each training instance, all symmetry-equivalent (orbit) Wyckoff assignments are enumerated up to a default cutoff N=64; for higher orbits, stratified random sampling is used (seed=42). For ambiguous/degenerate cases, sample scenarios (e.g., overlapping Wyckoff sets, partial site occupancy) are logged, with default rates reported; these cases are included in test/ablation sets but excluded from main training.
- All datasets (including ambiguous/edge-case splits) and configuration files are published alongside code.

2. Model Architecture:
- Architecture consists of two main branches:
   (i) a group-equivariant network (e.g., e3nn/LieConv) for space group operations;
   (ii) a permutation/set-equivariant network (Set Transformer/DeepSets) for Wyckoff site sets.
- Branch outputs are fused via cross-attention; embedding size=128, depth=8 layers (default in config.yaml).

3. Canonicalization & Ambiguity Protocol:
- Before model input and after generation, crystal representations are canonicalized using a two-stage algorithm:
   - (a) Group-theoretic invariants (site symmetry, multiplicity);
   - (b) Lexicographic Wyckoff resolution with prescribed tie-breakers (e.g., minimal total coordinate value, explicit enantiomorph handling).
- Pseudo-code and explicit flowcharts are in the main repo; all canonicalization failures or ambiguities trigger a fallback to probabilistic output or are routed to edge-case logs (with default exclusion from main metrics, but inclusion in robustness tests).

4. Training & Loss:
- Train with a marginalization loss: for each sample, minimize the average (Sinkhorn) set-to-set divergence between generated and all valid ground-truth Wyckoff-orbit sets; negative samples are constructed from other space groups and symmetry-invalid assignments.
- Batch augmentation and random group operations promote coverage.

5. Filtering & Validation:
- All outputs must pass hard-coded physical checks (min interatomic distance >1Å, charge neutrality, etc.), then fast forcefield scoring (e.g., OpenBabel), and, optionally, DFT-based energy validation.
- Robustness is quantified by injecting controlled coordinate noise and benchmarking on ambiguous/OOD cases.

6. Evaluation & Benchmarking:
- Compare against DiffCSP, FlowMM, and baseline canonical-Wyckoff models; report accuracy, novelty, stability, symmetry-faithfulness on both clean and ambiguous sets, using explicit thresholds and public protocols.

7. Openness & Reproducibility:
- Publish (a) code & model weights, (b) complete config files, (c) default datasets, (d) flowcharts/pseudo-code, (e) ambiguity rates & logs.

8. Novelty Positioning:
- The joint use of orbit-marginalized set-valued output, group-/permutation-equivariant architecture, and fully transparent handling of ambiguous Wyckoff assignments uniquely distinguishes this approach from recent symmetry-aware models.


Hypothesis ID: 15
Averaged Score: 4.0; Scores: [4, 3, 4, 5]
Number of rounds: 2
We hypothesize that a symmetry-faithful generative model for crystalline materials can be constructed by integrating (i) explicit Wyckoff position-based, discrete sequence generation with legality masking; (ii) group-equivariant neural networks for structure property prediction and latent fusion; and (iii) augmentation over inequivalent Wyckoff labelings, yielding generated structures faithfully matching real-world distributions of space groups while improving stability, uniqueness, and novelty.

Key methodology:

1. **Data Representation & Augmentation**:
   - Crystals are encoded as (a) space group (1–230, one-hot or embedding), (b) Wyckoff site sequences (discrete labels, padded to N_max), (c) atomic species per Wyckoff site, and (d) asymmetric unit coordinates.
   - For each training sample, enumerate all inequivalent Wyckoff representations consistent with its structure (using international tables and graph isomorphism checks). If enumerating all is infeasible (|classes| > T_max), stochastically sample/augment up to T_max labelings.
   - Data fields, masks, and augmentation procedures are logged for reproducibility; see Table S1 for hyperparameters and formats.

2. **Legality Masking & Sequence Generation**:
   - An autoregressive transformer with N_layers, N_heads per layer, and d_model (all hyperparameterized) outputs, at each step, the next Wyckoff site and species, masking predictions via legality matrices specific to current partial sequence and space group. Legality masks are precomputed for all space groups at initialization, then adapted per sample during decoding as assignments accumulate.
   - Partial occupancies, ambiguous labels, or rare/partial sites in training data trigger rejection sampling with a configurable maximum retry R_max (abandon on exceeding R_max). Missing/ambiguous sites are filled with sentinel values and excluded from loss computations.

3. **Equivariant Structure Branch & Fusion**:
   - An e3nn-based group-equivariant network encodes asymmetric unit coordinates and infers crystal property proxies. Cross-attention modules connect equivariant (coordinate) and transformer (sequence) branches at each layer, synchronizing symmetry representations and sequence predictions.
   - Fusion outputs are regulated by joint orbit-invariant loss terms: (a) permutation/Hungarian-matched RMSD on generated vs. reference atoms (for symmetry equivalence), and (b) space group/symmetry consistency loss.

4. **Training & Validation Protocols**:
   - Losses: Weighted sum of (i) cross-entropy (sequence prediction), (ii) orbit-invariant structure loss, and (iii) symmetry regularization (coefficients listed in Table S1); hyperparameter search performed on validation splits (documented seeds, splits).
   - ML-facilitated screening: A trained stability/classifier (see reference architecture in Appendix) pre-filters generated structures; a subset (p%) undergoes full DFT validation using reference settings (VASP, ENCUT, K-mesh).
   - Generation is restarted if legality/augmentation procedures block all choices or sample is degenerate; frequencies recorded for diagnostic reporting.

5. **Benchmarks, Diagnostics, and Reproducibility**:
   - Comparison metrics: symmetry faithfulness (space group/Wyckoff statistics), stability (ML & DFT), uniqueness vs. MP/ICSD, and coverage across chemical space groups.
   - Open-source implementation, data loader schema, legality mask computation pseudocode, and an evaluation benchmark suite (SymmetryGenBench) to be provided for transparent replication and extension.

All parameters, edge cases, fallbacks, and diagnostic metrics are fully specified.


Hypothesis ID: 16
Averaged Score: 4.0; Scores: [4, 4, 3, 5]
Number of rounds: 3
We hypothesize that a two-stage, symmetry-aware generative framework—comprising a legality-masked, canonical Wyckoff position sequence generator coupled to a conditional diffusion model for atom species and coordinates—can produce unique, valid, and realistic crystal structures faithfully preserving the distribution of symmetry space groups.

1. **Data Representation**: 
   - Represent crystals by (a) their canonical space group label and (b) a canonical, equivalence-class-augmented tuple of Wyckoff positions reflecting the asymmetric unit.
   - To ensure deduplication and cross-dataset integrity, implement group-theoretic and hand-vetted canonicalization algorithms. Ambiguities or unresolved cases are flagged in a version-controlled exception log.

2. **Legality Masking & Edge-case Handling**:
   - For each space group, precompute (and cache) legality masks for additive Wyckoff positions. At each autoregressive step, apply the current legality mask to restrict possible choices.
   - For rare or memory-intensive cases, employ a fallback: (i) similarity-based sampling from nearest populated groups; (ii) partial masking using prior data frequencies; (iii) manual override if all automatic procedures fail. All fallback instances are logged, and procedures are documented in explicit pseudocode.

3. **Model Architecture**:
   - Stage 1: A legality-masked transformer autoregressively generates the sequence of Wyckoff positions, conditioned on the space group. Group-theoretic legality masks are applied per step, with fallback logic.
   - Stage 2: The output sequence (via a learned embedding) conditions a diffusion model to generate atomic species and coordinates for the asymmetric unit, ensuring chemical and physical feasibility.
   - Shared latent vectors enable feedback from Stage 2 to Stage 1 during joint/fine-tuned training.

4. **Training Objectives & Augmentation**:
   - Employ contrastive, set-invariant loss: positive pairs are deduplicated pairs; hard negatives are distinct Wyckoff sets within isomorphic groups.
   - Rare group augmentation uses stratified oversampling, synthetic equivalence-class expansion, and legality-preserving group action perturbations. Each augmentation is filtered post-hoc by proxy ML stability classifiers and symmetry checks.

5. **Validation and Ablation**:
   - Baselines include non-symmetry-aware diffusion, transformer-only, and GAN/autoencoder variants.
   - Perform ablations on legality masking fidelity, canonicalization granularity, augmentation magnitude, and stability proxy selection.
   - All workflow steps (data prep, augmentation, mask computation, fallback, generation, and screening) are implemented with explicit, versioned scripts and pseudocode, reproducible with specified hardware/memory constraints.

6. **Benchmarking and Downstream Relevance**:
   - Detailed benchmarks on legality mask computation and memory for large/rare groups are performed and documented. For augmented/rare cases, additional human and ML validation ensures no synthetic structures violate physical plausibility or group assignment.
   - Protocols are version-controlled to track updates with new group-theoretic results or expanded datasets.


Hypothesis ID: 17
Averaged Score: 3.75; Scores: [4, 3, 3, 5]
Number of rounds: 1
Hypothesis: A generative pipeline that canonicalizes crystalline materials via group-theoretical Wyckoff representations and learns symmetry-informed structure generation using an equivariant GNN encoder and constraint-masked Transformer decoder will faithfully reproduce the distribution of symmetry space groups, reduce Wyckoff redundancy, and yield unique, stable samples.

Methodology:

1. Data Representation/Canonicalization:
- Input crystal structures (CIFs) from Materials Project/ICSD are processed using spglib or pymatgen to extract a unique canonical Wyckoff sequence per structure, resolving site permutations and equivalencies.
- Dataset is filtered to cells with ≤30 atomic sites; ambiguous or symmetry-imperfect structures (as judged by group-theoretical tolerance ε = 10−3 Å) are either separated out or processed via an enumeration fallback.

2. Model Architecture:
- Encoder: An SE(3)-Transformer (depth=8, width=256, 6 attention heads, 12 message passing steps) operates on site graph representation (species, Wyckoff label, fractional coordinates). Orbit pooling aggregates features for symmetry-equivalent orbits; the latent bottleneck dimension is 128.
- Decoder: An autoregressive, GPT-2-style Transformer (12 layers, width 256, 8 heads) predicts a legal sequence of Wyckoff letters and site occupancies, masked at each step by a precomputed legality table (based on group-subgroup compatibility and physical plausibility). Masking is performed statically for each group, with a dynamic fallback for rare ambiguous completions.

3. Training/Regularization:
- Dataset is deduplicated (structure match via pymatgen, symmetry tolerance ε).
- Cross-entropy loss is applied to the predicted Wyckoff and occupancy sequence, weighted 0.7. An additional distributional regularization term (KL-divergence between generated and real frequencies for space group/Wyckoff combinations), weighted 0.3, encourages global fidelity.
- Adam optimizer (lr=1e-4), batch size 64, early stopping based on validation KL-divergence.
- Data augmentation includes coordinate jitter (σ=0.02), Wyckoff sequence shuffling (when consistent with symmetry), and dropout (0.20) in both encoder/decoder.

4. Postprocessing/Evaluation:
- Generated structures are mapped back to CIFs via a standard inverse transformation pipeline (Wyckoff → atomic coordinates).
- Evaluation metrics:
  (a) Uniqueness: (1 - (# duplicate structures / # total generated)) via graph isomorphism matching over 1000 samples.
  (b) Symmetry fidelity: KL-divergence between model-generated and dataset space group distributions.
  (c) Wyckoff redundancy: Fraction of generated structures sharing the same physical crystal (after canonicalization) as another in the batch.
  (d) Stability: Fraction of generated structures passing automated geometry/DFT pre-filters.
- Ablations: Comparisons include (i) removal of canonicalization, (ii) non-equivariant encoder or decoder, (iii) without legality masking. All other hyperparameters held fixed for ablation clarity.

5. Computational Considerations:
- Pipeline runs on 32GB GPU, structure limit 30 sites. Decoding ~0.05s/sample. Legality mask lookup precomputed for all 230 space groups.
- For defective or non-canonicalizable data, pipeline enumerates alternative Wyckoff representations and selects the highest-likelihood completion (cross-validated on held-out data).

Novelty:
- Full-integration of orbit-pooling, group-canonicalization, and hard-masked autoregressive decoding for Wyckoff-based generative modeling is new.
- Distributional regularization for symmetry-fidelity has not been applied in this domain. Prior models lack either canonicalization, equivariant encoding, or legality-aware autoregressive decoding.




Hypothesis ID: 18
Averaged Score: 3.75; Scores: [4, 3, 3, 5]
Number of rounds: 1
**Hypothesis:**

A generative model for crystalline materials, trained and evaluated as follows, will faithfully reproduce the empirical distribution of symmetry space groups (as observed in MP-20) and outperform SOTA (DiffCSP, FlowMM, WyckoffSet, CrystalFormer) models in stability, uniqueness, and novelty.

**1. Data Curation and Representation:**
- Select solid-state compound entries from canonical databases (Materials Project, ICSD) filtered for: single-phase order, full occupancies, reliable symmetry assignment (via spglib), and ≤40 atoms/unit cell.
- Encode each structure as a canonicalized Wyckoff sequence:
  - Sort by space group, Wyckoff letter, site occupancy; apply explicit, public tie-breakers (detailed in supplementary pseudocode) for ambiguous assignments, referencing accepted conventions (e.g., Bilbao Crystallographic Server) but ensuring all rules are reimplemented for reproducibility.
  - Enumerate all equivalent Wyckoff representations, mapping to a unique canonical form for each equivalence class.

**2. Model Architecture:**
- Use a transformer-based autoregressive generator conditioned on space group.
- At each step, input the partial Wyckoff sequence; output next symbol (or site species) using real-time legality masking based on current partial assignment.
- Real-time legality mask:
  - Constructed as a composite logical AND of chemical (charge neutrality: ±1e; species–Wyckoff compatibility; ionic radii: per tabulated cutoffs) and physical (site occupancy, minimum interatomic distance >1.5 Å; preserved group symmetry) constraints; soft/probabilistic masking applied where site-assignment ambiguity exists (e.g., variable oxidation states).
  - Lookup tables for ionic radii, oxidation states, and allowed site coordinations are explicitly referenced (provided in supplementary materials).
  - Pre-compute legality masks for frequent space group/Wyckoff pairs; implement caching to speed generation.

**3. Canonicalization and Set-Invariant Loss:**
- Learning objective is permutation-invariant: model is trained to minimize cross-entropy loss over the equivalence class for each crystal, i.e., loss is computed over all orderings mapped to the canonical Wyckoff sequence (see explicit formula in supplementary). This enforces invariance to ordering and redundancy.

**4. Training and Fallback Protocols:**
- Training/validation/test splits are stratified by space group (algorithm detailed in supplement).
- If legality masking results in a dead end (no valid moves), fallback protocol relaxes constraints in a ranked order (first radii, then charge, then symmetry), with each relaxation tracked per sample; threshold limits are specified (e.g., radii relaxed up to +10%, charge up to ±2e).
- Optionally, issue a diagnostic if >20% of generations require fallback or record fallback rate in evaluation.

**5. Evaluation and Benchmarking:**
- Output samples benchmarked versus SOTA on: (a) symmetry fidelity (space group matching with structure inspector), (b) uniqueness (fraction of unique Wyckoff classes), (c) redundancy (repeats within generated set), and (d) stability (ML proxy, e.g., OQMD GNN ≥ conventional threshold, and spot-checked DFT energies).
- All code, pseudocode (for canonicalization, masking, fallback logic), data splits, and lookup tables are open-sourced for full reproducibility.

This modular, canonical, legality-masked, and permutation-invariant framework constitutes a reproducible, novel generative approach for crystal structure design.


Hypothesis ID: 19
Averaged Score: 3.5; Scores: [4, 3, 3, 4]
Number of rounds: 1
We hypothesize that a transformer-based autoregressive generative model utilizing stepwise canonicalized Wyckoff position sequences, combined with explicit multi-domain chemical and symmetry constraint masking during decoding, will yield higher rates of unique, symmetric, and chemically valid crystalline materials compared to previous SOTA models.

Methodology:

1. Data Encoding and Canonical Wyckoff Representation
- Each crystal in the dataset is represented as an ordered sequence of (space group, Wyckoff letter, multiplicity, atomic species).
- Canonicalization is strictly defined: (a) Wyckoff sites are ordered first by decreasing multiplicity, then alphabetically, then by atomic species lex order; (b) For symmetry-equivalent arrangements, the lexicographically minimal permutation is selected using atomic Z ordering as final tie-breaker.
- When multiple valid equivalences arise (e.g., for high-symmetry space groups), the canonical label is uniquely assigned via recursive mapping to the minimal site label list, not relying on external libraries.
- Detailed pseudocode for the canonicalization protocol is provided, including handling edge cases and ambiguous site assignments.

2. Real-Time Constraint Masking During Autoregressive Decoding
- At each token generation step, output candidates (atomic species, Wyckoff letter, site occupancy) are filtered using a masking function integrating:
    * Chemical valency compatibility: Dynamic oxidation state tracker updates charge with each assignment, enforcing global electroneutrality and using ICSD/Mp-inferred oxidation states.
    * Structural compatibility: Space group/Wyckoff occupation rules from Bilbao server/manual tables.
    * Geometric feasibility: Atomic radius sum constraints (Shannon radii, <10% over/under tolerance) for neighbor pairs considered at each partial structure step.
    * Site preference: Empirical rules per IUPAC and literature, e.g., transition metals in octahedral sites (pre-encoded by site symmetry label), with curator-defined adjustable thresholds.
- Masking remains modular, with tunable strictness, logged masking rates, and mask/proposal ratio tracked for each class/space group.

3. Dead-End Handling and Sampling Logic
- If the candidate mask is empty (dead-end), rollback occurs: beam search with empirically-tuned width (e.g., 32–256) is deployed, prioritizing high-likelihood prefix continuations; generation retries are capped at N=4 per structure (configurable).
- All generation attempts and rollbacks are logged for error analysis; no invalid structure is finalized.

4. Training Routine and Augmentation
- Model is trained with canonicalized sequences only; label permutation augmentation is limited to synthetic non-canonical orderings which are reverted for loss computation, guaranteeing unambiguous mapping between input and canonical target.
- Auxiliary loss for rule violation is set to a weight of 0.2 (tunable) and makes use of constraint violation counters updated at each decoding step.

5. Distinction from Prior Models
- Unlike prior works, constraint masking and canonicalization operate strictly at runtime in the autoregressive loop, not via pre/post-processing. Tabulated protocol comparisons are provided.

Codebase for canonicalization and masking modules will be supplied for community transparency and validation.


Hypothesis ID: 20
Averaged Score: 3.25; Scores: [3, 3, 3, 4]
Number of rounds: 2
---
**Hypothesis:**
A generative model for crystalline solids using canonicalized Wyckoff representations and explicit space group conditioning can be constructed with the following methodological pipeline, yielding symmetry-faithful, stable, unique, and novel materials superior to existing SOTA.

**1. Canonical Data Representation**
- For each dataset crystal, obtain (via spglib, symprec=1e-5) the tuple: (space group number g, sorted Wyckoff letter sequence w, site multiplicities m, atomic species a, fractional occupations f).
- Enumerate all crystallographically equivalent Wyckoff encodings (using symmetry generators and allowed label permutations).
- Select the lex minimal canonical tuple (g, w, m, a, f) as the unique representation. For structures with partial occupancies/disorder:
    - If f < threshold (e.g., 0.2), omit the site; else merge into composite labels with fractional mask; else flag for separate optional modeling.
- Store all canonical tuples for uniqueness computation and buffer updates.

**2. Space Group-aware Model Architecture**
- Input: sequence of canonical tuples; augment with one-hot and learned continuous embeddings e_g for the 230 space groups.
- For each token (site), concatenate e_g, Wyckoff embedding, atomic species and mask.
- For space groups with <T data points (T≈150), use group-generator adjacency matrices as additional input, initialized from standard tables (binary or weighted by geometric relevance). All group embeddings are trainable.
- Architecture: (a) A transformer backbone operating on the sequence, with (b) group-specific adapter modules for frequent groups, else universal backbone. (c) SE(3)-equivariant message-passing (e.g., tensor field networks) applied at site and full configuration level.

**3. Training and Objective Functions**
- Symmetry-Orbit-Invariant Loss: Given reference (Y) and prediction (Ŷ), match sites using Hungarian algorithm on RMSD(w, a, f), with symmetry-generated equivalence mapping.
- EMD Loss: Match log-batched histograms of generated vs. training space group distributions (Sinkhorn iteration, λ=0.01, defined cost matrix), batch size B (specify).
- Uniqueness Regularizer: Maintain a FIFO buffer of prior canonical tuples; penalize duplicates via cross-batch uniqueness loss.
- Surrogate Property Loss: Predict properties (formation energy, dynamical stability) with auxiliary MLP, loss propagated using DFT labels or ML surrogates.
- Self-supervision for rare groups: contrastive loss between real/augmented rare group representations (positive: same group under augmentation; negative: different group), with explicit sampling routines. Rare group data augmented using parameter-mixed interpolation or group-morphing via generator matrices; fallback: mixup or domain-adaptation auxiliary heads.

**4. Hierarchical Validation and Routing**
- Physical plausibility filters: Charge neutrality, distance checks, symmetry completeness.
- If model/ensemble uncertainty <U₁, route to DFT/phonon screening; else if <U₂, assess by ML surrogates; else discard.
- Document and analyze flows for rare group candidates and severe disorder separately.

**5. Benchmarking and Open Protocols**
- Use MP-20, with stratified group splits and ablations (group adapters, rare group augmentation, canonicalization).
- Baselines: Score-based/flow models, prior Wyckoff/symmetry models (3–5 strictly named SOTA).
- Metrics: (i) symmetry match rate, (ii) uniqueness (buffer novelty, chemical equivalence), (iii) stability, (iv) EMD distance.
- Release code for canonicalization/minimization, group representations, and all leaderboards.
---


Hypothesis ID: 21
Averaged Score: 3.25; Scores: [3, 3, 3, 4]
Number of rounds: 3
We hypothesize that a generative model based on canonicalized Wyckoff representations, integrating deterministic legal site-masking, group-conditional modular neural architectures, advanced meta-learning for rare space groups, and scalable tiered physical validation, can robustly learn and reproduce the true distribution of crystal space groups, yielding stable, unique, and novel inorganic structures beyond state-of-the-art baselines.

Methodology:

1. Data Representation & Canonicalization
- Structures are encoded as sequences of Wyckoff-site tuples: (space group number, Wyckoff symbol, element type, occupancy, free coordinates).
- For each input, generate all symmetry-equivalent Wyckoff assignments using spglib (v2.0.2) and select a unique canonical form utilizing Weisfeiler-Lehman graph canonicalization (networkx v3.2), with deterministic lex-min fallback and padding protocol for incomplete sites.
- Partial occupancies and disorder are handled by encoding as separate categorical states, flagged for downstream filtering.

2. Legal Site Masking
- Implement a universal site-masking function that, given a space group (International Tables v2021 JSON schema), generates valid Wyckoff occupancy masks at every autoregressive step. Pseudocode and sample implementations provided (see Supplement Section S1), with verification against representative rare/degenerate cases.

3. Model Architecture
- Employ a Transformer- or Diffusion-based generative model with a group-conditional modular design: a backbone shared across all space groups, and group-specific heads for space group-dependent decoding.
- Meta-learning module (e.g., Prototypical Networks, inner/outer LRs=1e-3/1e-4, 5 adaptation steps) enables adaptation to rare (<20 samples) space groups, with criteria for method selection justified via cross-validation.
- Losses include: negative log-likelihood, Earth Mover’s Distance for direct distributional matching of space group frequencies, symmetry violation penalties, and property-aligned auxiliary losses.

4. Training & Augmentation
- Stratified sampling/augmentation ensures balanced coverage, with compositional/textured augmentations for node features and Wyckoff permutations.
- Mixed-precision (fp16) training, batch size 256, AdamW (lr=5e-4, wd=1e-2) on NVIDIA A100 hardware (compute demand profiling given in Supplement S2).

5. Post-Generation Canonicalization & Validation
- All sampled structures decoded and re-canonicalized, with duplicates filtered via SOAP (cutoff 5Å) and/or ACSF (rcut=6Å, n=8, l=6) fingerprints; uniqueness and novelty scoring calibrated at cosine <0.1 (validated in Supplement S3).
- Validation is tiered: ML-based stability screening (e.g., CGCNN), followed by queue-limited DFT (VASP/PBE, PAW_PBE v.54, Ecut=520eV, 4x4x4 k-grid) and phonon (Phonopy v2.17).
- For rare groups, transfer learning from proximate high-data groups, and/or synthetic oversampling is used as fallback.

6. Novelty and Reproducibility
- Tabulated novelty/contrast with recent work (see Supplement Table S4) provided.
- Source code (Python 3.11, torch v2.2, spglib v2.0, pymatgen v2023.9) and full validation setups to be openly released for community benchmarking and extension.

Each module and threshold in this pipeline is fully specified and justified, enabling comparative and reproducible assessment at every stage.


Hypothesis ID: 22
Averaged Score: 2.75; Scores: [3, 2, 2, 4]
Number of rounds: 2
We hypothesize that an end-to-end generative system built on canonicalized, group-encoded Wyckoff representations with dynamic legality masking and group-conditional transformer heads, trained using set-invariant losses and empirical group distribution matching, will enable high-fidelity crystal structure generation faithful to real space group statistics and possessing higher stability and uniqueness than SOTA models.

Methodology outline:

**Data Representation:**
- Each crystal structure is converted to a canonical Wyckoff representation via spglib, using lexicographically minimal Wyckoff letter/configuration order. Ambiguity in Wyckoff ordering is resolved by lex-minimization and permutation pruning; all equivalent forms are deduplicated. Each sample stores explicit group label, sequence of Wyckoff sites, and associated elements.

**Legality Masking:**
- At each autoregressive step, a legality mask over the next Wyckoff position/species is computed using pymatgen-based rules conditioned on partial sequence, group constraints, composition, and site occupancy—mask tensors have shape (batch, group_id, site_idx, option_dim). Masks are cached per group and updated dynamically.
- Fallback policy: If no valid option exists (mask is all zeros), sample is aborted and retried; a penalty is applied during training for such dead-ends. Optionally, partial backtracking is explored.

**Model Architecture:**
- A transformer backbone encodes sequence context (input: batch × seq_len × [Wyckoff, species]), with separate group-conditional output heads (one per group or group cluster), sharing all but the final layers. Layers: (L=6, d_model=512, 8 heads per group, hyperparameters tunable). Rare groups (<N_min samples) use adapter fusion into main heads.
- Input/output tensor formats and mask propagation: Inputs and masks are stacked along batch and group axes; group mask is toggled based on sample’s group label.

**Training and Losses:**
- Set-invariant loss: Outputs are evaluated over all (or Monte-Carlo subsampled) site permutations consistent with symmetry, via explicit permutation augmentation in each batch (limited to max_n_perm per sample for tractability).
- Distribution matching: An explicit loss term (EMD/KL) penalizes deviation between model-generated and empirical space group distributions in batch.
- Auxiliary losses: Property predictor (e.g., pre-trained M3GNet/CGCNN surrogate) for stability guides joint training; uniqueness loss penalizes duplicate generations.

**Data Augmentation & Filtering:**
- Stratified batch sampling ensures balanced group representation. Synthetic samples via symmetry augmentation/SMOTE if group frequency is low. Hyperparameters: batch_size=128, max_n_perm=16, group_min_samp=100.

**Validation & Benchmarking:**
- Generated crystals are first filtered by legality; stability screening via surrogate (M3GNet, cutoff -0.2 eV/atom), with selected high-performing candidates DFT-relaxed. Benchmarking is done on MP-20, assessing group distribution match, uniqueness, and structure validity. Ablations include legality mask removal, permutation augmentation scope, rare-group handling.

**Reproducibility:**
- All scripts (canonicalization, legality mask, model, ablation/analysis) are open-sourced, each with test sets and config files for full reproducibility.


